{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frontmatter",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Text Analysis for Qualitative Research\"\n",
    "output-file: \"text_analysis.html\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìù 6.2 Text Analysis for Qualitative Research\n",
    "\n",
    "This notebook introduces text analysis techniques for qualitative nutrition research, focusing on processing survey responses.\n",
    "\n",
    "**Objectives**:\n",
    "- Preprocess text data using tokenization and stopword removal.\n",
    "- Perform word frequency analysis and visualization.\n",
    "- Apply techniques to `food_preferences.txt` to uncover hippo dietary preferences.\n",
    "\n",
    "**Context**: Qualitative analysis of survey data, like hippo food preferences, reveals insights into dietary behaviours, complementing quantitative methods.\n",
    "\n",
    "<details><summary>Fun Fact</summary>\n",
    "Hippos love to express their food preferences, and text analysis helps us decode their crunchy cravings! ü¶õ\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text analysis environment ready.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for text analysis\n",
    "%pip install nltk pandas  # Ensures compatibility in Colab\n",
    "import nltk  # For natural language processing\n",
    "import pandas as pd  # For data manipulation\n",
    "from nltk.tokenize import word_tokenize  # For splitting text into words\n",
    "from nltk.corpus import stopwords  # For removing common words\n",
    "from collections import Counter  # For counting word frequencies\n",
    "import matplotlib.pyplot as plt  # For visualization\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('stopwords')  # Stopwords list\n",
    "print('Text analysis environment ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Load `food_preferences.txt`, containing 50 hippo survey responses, and preprocess the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of responses: 50\n",
      "Sample response: Hippo H1: I enjoy crunchy carrots.\n"
     ]
    }
   ],
   "source": [
    "# Load survey responses\n",
    "with open('data/food_preferences.txt', 'r') as file:\n",
    "    responses = [line.strip() for line in file]\n",
    "\n",
    "print(f'Number of responses: {len(responses)}')  # Display total responses\n",
    "print(f'Sample response: {responses[0]}')  # Show first response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Tokenize responses, convert to lowercase, and remove stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "preprocess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens from first response: ['enjoy', 'crunchy', 'carrots']\n"
     ]
    }
   ],
   "source": [
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english')).union({':', '.', 'hippo', 'i'})\n",
    "\n",
    "# Tokenize and clean responses\n",
    "tokens = []\n",
    "for response in responses:\n",
    "    words = word_tokenize(response.lower())  # Convert to lowercase and tokenize\n",
    "    clean_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    tokens.extend(clean_words)\n",
    "\n",
    "print(f'Sample tokens from first response: {tokens[:3]}')  # Show first few tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequency",
   "metadata": {},
   "source": [
    "## Word Frequency Analysis\n",
    "\n",
    "Count and visualize the most common words in the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "frequency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words: [('carrots', 15), ('crunchy', 12), ('sweet', 10), ('enjoy', 8), ('greens', 7)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count word frequencies\n",
    "word_freq = Counter(tokens)\n",
    "top_words = word_freq.most_common(5)\n",
    "print(f'Top 5 words: {top_words}')\n",
    "\n",
    "# Visualize word frequencies\n",
    "words, counts = zip(*top_words)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(words, counts)\n",
    "plt.title('Top 5 Words in Hippo Food Preferences')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()  # Display bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise",
   "metadata": {},
   "source": [
    "## Exercise: Analyze Adjectives\n",
    "\n",
    "Modify the preprocessing to extract only adjectives (e.g., 'crunchy', 'sweet') and count their frequencies. Visualize the top 3 adjectives in a bar plot.\n",
    "\n",
    "**Guidance**:\n",
    "- Use NLTK‚Äôs part-of-speech tagging (`nltk.pos_tag`) with `nltk.download('averaged_perceptron_tagger')`.\n",
    "- Filter for adjectives (POS tag 'JJ').\n",
    "- Create a bar plot of the top 3 adjectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "your_answer",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "My adjective analysis code and results are as follows:\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```\n",
    "\n",
    "**Top 3 Adjectives**:\n",
    "\n",
    "- [Adjective 1]: [Count]\n",
    "- [Adjective 2]: [Count]\n",
    "- [Adjective 3]: [Count]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrap",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You‚Äôve applied text analysis to uncover dietary preferences from hippo survey responses, revealing key themes like 'crunchy carrots'.\n",
    "\n",
    "**Next Steps**: Apply these skills to your own qualitative datasets or revisit earlier modules for quantitative analysis.\n",
    "\n",
    "**Resources**:\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [Text Analysis Tutorial](https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk)\n",
    "- Repository: [github.com/ggkuhnle/data-analysis-toolkit-FNS](https://github.com/ggkuhnle/data-analysis-toolkit-FNS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
